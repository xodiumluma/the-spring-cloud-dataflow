[[configuration]]
= Configuration

[[configuration-maven]]
== Maven Resources
Spring Cloud Dataflow supports referencing artifacts via Maven (`maven:`).
If you want to override specific Maven configuration properties (remote repositories, proxies, and others) or run the Data Flow Server behind a proxy,
you need to specify those properties as command-line arguments when you start the Data Flow Server, as shown in the following example:

====
[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-server-{project-version}.jar --spring.config.additional-location=/home/joe/maven.yml
----
====

The preceding command assumes a `maven.yaml` similar to the following:

====
[source,yml]
----
maven:
  localRepository: mylocal
  remote-repositories:
    repo1:
      url: https://repo1
      auth:
        username: user1
        password: pass1
      snapshot-policy:
        update-policy: daily
        checksum-policy: warn
      release-policy:
        update-policy: never
        checksum-policy: fail
    repo2:
      url: https://repo2
      policy:
        update-policy: always
        checksum-policy: fail
  proxy:
    host: proxy1
    port: "9010"
    auth:
      username: proxyuser1
      password: proxypass1
----
====

By default, the protocol is set to `http`. You can omit the auth properties if the proxy does not need a username and password. Also, by default, the maven `localRepository` is set to `${user.home}/.m2/repository/`.
As shown in the preceding example, you can specify the remote repositories along with their authentication (if needed). If the remote repositories are behind a proxy, you can specify the proxy properties, as shown in the preceding example.

You can specify the repository policies for each remote repository configuration, as shown in the preceding example.
The key `policy` is applicable to both the `snapshot` and the `release` repository policies.

See the https://github.com/apache/maven-resolver/blob/master/maven-resolver-api/src/main/java/org/eclipse/aether/repository/RepositoryPolicy.java[Repository Policies] topic for the list of
supported repository policies.

As these are Spring Boot `@ConfigurationProperties` you need to specify by adding them to the `SPRING_APPLICATION_JSON` environment variable. The following example shows how the JSON is structured:

====
[source,bash,subs=attributes]
----
$ SPRING_APPLICATION_JSON='
{
  "maven": {
    "local-repository": null,
    "remote-repositories": {
      "repo1": {
        "url": "https://repo1",
        "auth": {
          "username": "repo1user",
          "password": "repo1pass"
        }
      },
      "repo2": {
        "url": "https://repo2"
      }
    },
    "proxy": {
      "host": "proxyhost",
      "port": 9018,
      "auth": {
        "username": "proxyuser",
        "password": "proxypass"
      }
    }
  }
}
'
----
====

=== Wagon

There is a limited support for using `Wagon` transport with Maven. Currently, this
exists to support _preemptive_ authentication with `http`-based repositories
and needs to be enabled manually.

Wagon-based `http` transport is enabled by setting the `maven.use-wagon` property
to `true`. Then you can enable _preemptive_ authentication for each remote
repository. Configuration loosely follows the similar patterns found in
https://maven.apache.org/guides/mini/guide-http-settings.html[HttpClient HTTP Wagon].
At the time of this writing, documentation in Maven's own site is slightly misleading
and missing most of the possible configuration options.

The `maven.remote-repositories.<repo>.wagon.http` namespace contains all Wagon
`http` related settings, and the keys directly under it map to supported `http` methods --
namely, `all`, `put`, `get` and `head`, as in Maven's own configuration.
Under these method configurations, you can then set various options, such as
`use-preemptive`. A simpl _preemptive_ configuration to send an auth
header with all requests to a specified remote repository would look like the following example:

====
[source,yml]
----
maven:
  use-wagon: true
  remote-repositories:
    springRepo:
      url: https://repo.example.org
      wagon:
        http:
          all:
            use-preemptive: true
      auth:
        username: user
        password: password
----
====

Instead of configuring `all` methods, you can tune settings for `get`
and `head` requests only, as follows:

====
[source,yml]
----
maven:
  use-wagon: true
  remote-repositories:
    springRepo:
      url: https://repo.example.org
      wagon:
        http:
          get:
            use-preemptive: true
          head:
            use-preemptive: true
            use-default-headers: true
            connection-timeout: 1000
            read-timeout: 1000
            headers:
              sample1: sample2
            params:
              http.socket.timeout: 1000
              http.connection.stalecheck: true
      auth:
        username: user
        password: password
----
====

There are settings for `use-default-headers`, `connection-timeout`,
`read-timeout`, request `headers`, and HttpClient `params`. For more about parameters,
see https://github.com/apache/maven-wagon/blob/master/wagon-providers/wagon-http-shared/src/main/java/org/apache/maven/wagon/shared/http/ConfigurationUtils.java[Wagon ConfigurationUtils].

[[configuration-security]]
== Security

By default, the Data Flow server is unsecured and runs on an unencrypted HTTP connection.
You can secure your REST endpoints as well as the Data Flow Dashboard by enabling HTTPS
and requiring clients to authenticate with https://oauth.net/2/[OAuth 2.0].

[NOTE]
====
Appendix <<appendix-identity-provider-azure>> contains more information how to
setup _Azure Active Directory_ integration.
====

[NOTE]
====
By default, the REST endpoints (administration, management, and health) as well as the Dashboard UI do not require authenticated access.
====

While you can theoretically choose any OAuth provider in conjunction with
Spring Cloud Data Flow, we recommend using the
https://github.com/cloudfoundry/uaa[CloudFoundry User Account and Authentication (UAA) Server].

Not only is the UAA OpenID certified and is used by Cloud Foundry, but you can
also use it in local stand-alone deployment scenarios. Furthermore, the UAA not
only provides its own user store, but it also provides comprehensive LDAP integration.

[[configuration-local-security-enabling-https]]
==== Enabling HTTPS

By default, the dashboard, management, and health endpoints use HTTP as a transport.
You can switch to HTTPS by adding a certificate to your configuration in
`application.yml`, as shown in the following example:

====
[source,yaml]
----
server:
  port: 8443                                         # <1>
  ssl:
    key-alias: yourKeyAlias                          # <2>
    key-store: path/to/keystore                      # <3>
    key-store-password: yourKeyStorePassword         # <4>
    key-password: yourKeyPassword                    # <5>
    trust-store: path/to/trust-store                 # <6>
    trust-store-password: yourTrustStorePassword     # <7>
----

<1> As the default port is `9393`, you may choose to change the port to a more common HTTPs-typical port.
<2> The alias (or name) under which the key is stored in the keystore.
<3> The path to the keystore file. You can also specify classpath resources, by using the classpath prefix - for example: `classpath:path/to/keystore`.
<4> The password of the keystore.
<5> The password of the key.
<6> The path to the truststore file. You can also specify classpath resources, by using the classpath prefix - for example: `classpath:path/to/trust-store`
<7> The password of the trust store.
====

NOTE: If HTTPS is enabled, it completely replaces HTTP as the protocol over
which the REST endpoints and the Data Flow Dashboard interact. Plain HTTP requests
fail. Therefore, make sure that you configure your Shell accordingly.

[[configuration-security-self-signed-certificates]]
===== Using Self-Signed Certificates

For testing purposes or during development, it might be convenient to create self-signed certificates.
To get started, execute the following command to create a certificate:

====
[source,bash]
----
$ keytool -genkey -alias dataflow -keyalg RSA -keystore dataflow.keystore \
          -validity 3650 -storetype JKS \
          -dname "CN=localhost, OU=Spring, O=Pivotal, L=Kailua-Kona, ST=HI, C=US"  # <1>
          -keypass dataflow -storepass dataflow
----
<1> `CN` is the important parameter here. It should match the domain you are trying to access - for example, `localhost`.
====

Then add the following lines to your `application.yml` file:

====
[source,yaml]
----
server:
  port: 8443
  ssl:
    enabled: true
    key-alias: dataflow
    key-store: "/your/path/to/dataflow.keystore"
    key-store-type: jks
    key-store-password: dataflow
    key-password: dataflow
----
====

This is all you need to do for the Data Flow Server. Once you start the server,
you should be able to access it at `https://localhost:8443/`.
As this is a self-signed certificate, you should hit a warning in your browser, which
you need to ignore.

WARNING: _Never_ use self-signed certificates in production.

[[configuration-security-self-signed-certificates-shell]]
===== Self-Signed Certificates and the Shell

By default, self-signed certificates are an issue for the shell, and additional steps
are necessary to make the shell work with self-signed certificates. Two options
are available:

* Add the self-signed certificate to the JVM truststore.
* Skip certificate validation.

====== Adding the Self-signed Certificate to the JVM Truststore

In order to use the JVM truststore option, you need to
export the previously created certificate from the keystore, as follows:

====
[source,bash]
----
$ keytool -export -alias dataflow -keystore dataflow.keystore -file dataflow_cert -storepass dataflow
----
====

Next, you need to create a truststore that the shell can use, as follows:

====
[source,bash]
----
$ keytool -importcert -keystore dataflow.truststore -alias dataflow -storepass dataflow -file dataflow_cert -noprompt
----
====

Now you are ready to launch the Data Flow Shell with the following JVM arguments:

====
[source,bash,subs=attributes]
----
$ java -Djavax.net.ssl.trustStorePassword=dataflow \
       -Djavax.net.ssl.trustStore=/path/to/dataflow.truststore \
       -Djavax.net.ssl.trustStoreType=jks \
       -jar spring-cloud-dataflow-shell-{project-version}.jar
----
====

[TIP]
====
If you run into trouble establishing a connection over SSL, you can enable additional
logging by using and setting the `javax.net.debug` JVM argument to `ssl`.
====

Do not forget to target the Data Flow Server with the following command:

====
[source,bash]
----
dataflow:> dataflow config server --uri https://localhost:8443/
----
====

====== Skipping Certificate Validation

Alternatively, you can also bypass the certification validation by providing the
optional `--dataflow.skip-ssl-validation=true` command-line parameter.

If you set this command-line parameter, the shell accepts any (self-signed) SSL
certificate.

[WARNING]
====
If possible, you should avoid using this option. Disabling the trust manager
defeats the purpose of SSL and makes your application vulnerable to man-in-the-middle attacks.
====

[[configuration-security-oauth2]]
==== Authentication by using OAuth 2.0

To support authentication and authorization, Spring Cloud Data
Flow uses https://oauth.net/2/[OAuth 2.0].
It lets you integrate Spring Cloud Data Flow into Single Sign On (SSO)
environments.

NOTE: As of Spring Cloud Data Flow 2.0, OAuth2 is the only mechanism
for providing authentication and authorization.

The following OAuth2 Grant Types are used:

* *Authorization Code*: Used for the GUI (browser) integration. Visitors are redirected to your OAuth Service for authentication
* *Password*: Used by the shell (and the REST integration), so visitors can log in with username and password
* *Client Credentials*: Retrieves an access token directly from your OAuth provider and passes it to the Data Flow server by using the Authorization HTTP header

NOTE: Currently, Spring Cloud Data Flow uses opaque tokens and not transparent
tokens (JWT).

You can access the REST endpoints in two ways:

* *Basic authentication*, which uses the _Password Grant Type_ to authenticate with your OAuth2 service
* *Access token*, which uses the Client _Credentials Grant Type_

NOTE: When you set up authentication, you really should enable HTTPS
as well, especially in production environments.

You can turn on OAuth2 authentication by adding the following to `application.yml` or by setting
environment variables. The following example shows the minimal setup needed for
https://github.com/cloudfoundry/uaa[CloudFoundry User Account and Authentication (UAA) Server]:

====
[source,yaml]
----
spring:
  security:
    oauth2:                                                           # <1>
      client:
        registration:
          uaa:                                                        # <2>
            client-id: myclient
            client-secret: mysecret
            redirect-uri: '{baseUrl}/login/oauth2/code/{registrationId}'
            authorization-grant-type: authorization_code
            scope:
            - openid                                                  # <3>
        provider:
          uaa:
            jwk-set-uri: http://uaa.local:8080/uaa/token_keys
            token-uri: http://uaa.local:8080/uaa/oauth/token
            user-info-uri: http://uaa.local:8080/uaa/userinfo    # <4>
            user-name-attribute: user_name                            # <5>
            authorization-uri: http://uaa.local:8080/uaa/oauth/authorize
      resourceserver:
        opaquetoken:
          introspection-uri: http://uaa.local:8080/uaa/introspect # <6>
          client-id: dataflow
          client-secret: dataflow
----

<1> Providing this property activates OAuth2 security.
<2> The provider ID. You can specify more than one provider.
<3> As the UAA is an OpenID provider, you must at least specify the `openid` scope.
    If your provider also provides additional scopes to control the role assignments,
    you must specify those scopes here as well.
<4> OpenID endpoint. Used to retrieve user information such as the username. Mandatory.
<5> The JSON property of the response that contains the username.
<6> Used to introspect and validate a directly passed-in token. Mandatory.
====

You can verify that basic authentication is working properly by using curl, as follows:

====
[source,bash]
----
curl -u myusername:mypassword http://localhost:9393/ -H 'Accept: application/json'
----
====

As a result, you should see a list of available REST endpoints.

IMPORTANT: When you access the Root URL with a web browser and
security enabled, you are redirected to the Dashboard UI. To see the
list of REST endpoints, specify the `application/json` `Accept` header. Also be sure
to add the `Accept` header by using tools such as
https://chrome.google.com/webstore/detail/postman/fhbjgbiflinjbdggehcddcbncdddomop?hl=en[Postman] (Chrome)
or https://addons.mozilla.org/en-GB/firefox/addon/restclient/[RESTClient] (Firefox).

Besides Basic Authentication, you can also provide an access token, to
access the REST API. To do so, retrieve an
OAuth2 Access Token from your OAuth2 provider and pass that access token to
the REST Api by using the *Authorization* HTTP header, as follows:

```
$ curl -H "Authorization: Bearer <ACCESS_TOKEN>" http://localhost:9393/ -H 'Accept: application/json'
```

[[configuration-security-customizing-authorization]]
==== Customizing Authorization

The preceding content mostly deals with authentication -- that is, how to assess
the identity of the user. In this section, we discuss the available
*authorization* options -- that is, who can do what.

The authorization rules are defined in `dataflow-server-defaults.yml` (part of
the Spring Cloud Data Flow Core module).

Because the determination of security roles is environment-specific,
Spring Cloud Data Flow, by default, assigns all roles to authenticated OAuth2
users. The `DefaultDataflowAuthoritiesExtractor` class is used for that purpose.

Alternatively, you can have Spring Cloud Data Flow map OAuth2 scopes to Data Flow roles by
setting the boolean property `map-oauth-scopes` for your provider to `true` (the default is `false`).
For example, if your provider's ID is `uaa`, the property would be
`spring.cloud.dataflow.security.authorization.provider-role-mappings.uaa.map-oauth-scopes`.

[[configuration-security-role-mapping]]
===== Role Mappings

By default all roles are assigned to users that login to Spring Cloud Data Flow.
However, you can set the property:

`spring.cloud.dataflow.security.authorization.provider-role-mappings.uaa.map-oauth-scopes: true`

This will instruct the underlying `DefaultAuthoritiesExtractor` to map
OAuth scopes to the respective authorities. The following scopes are supported:

* Scope `dataflow.create` maps to the `CREATE` role
* Scope `dataflow.deploy` maps to the `DEPLOY` role
* Scope `dataflow.destroy` maps to the `DESTROY` role
* Scope `dataflow.manage` maps to the `MANAGE` role
* Scope `dataflow.modify` maps to the `MODIFY` role
* Scope `dataflow.schedule` maps to the `SCHEDULE` role
* Scope `dataflow.view` maps to the `VIEW` role

Additionally you can also map arbitrary scopes to each of the Data Flow roles:

[source,yaml]
----
spring:
  cloud:
    dataflow:
      security:
        authorization:
          provider-role-mappings:
            uaa:
              map-oauth-scopes: true                                    # <1>
              role-mappings:
                ROLE_CREATE: dataflow.create                            # <2>
                ROLE_DEPLOY: dataflow.deploy
                ROLE_DESTROY: dataflow.destoy
                ROLE_MANAGE: dataflow.manage
                ROLE_MODIFY: dataflow.modify
                ROLE_SCHEDULE: dataflow.schedule
                ROLE_VIEW: dataflow.view
----

<1> Enables explicit mapping support from OAuth scopes to Data Flow roles
<2> When role mapping support is enabled, you must provide a mapping for
all 7 Spring Cloud Data Flow roles *ROLE_CREATE*, *ROLE_DEPLOY*, *ROLE_DESTROY*, *ROLE_MANAGE*, *ROLE_MODIFY*, *ROLE_SCHEDULE*, *ROLE_VIEW*.

[TIP]
====
You can assign an OAuth scope to multiple Spring Cloud Data Flow roles, giving you flexible regarding the granularity of your authorization configuration.
====

[[configuration-security-group-mapping]]
===== Group Mappings

Mapping roles from scopes has its own problems as it may not be always possible
to change those in a given identity provider. If it's possible to define group claims
in a token returned from an identity provider, these can be used as well to
map into server roles.

====
[source,yaml]
----
spring:
  cloud:
    dataflow:
      security:
        authorization:
          provider-role-mappings:
            uaa:
              map-oauth-scopes: false
              map-group-claims: true
              group-claim: roles
              group-mappings:
                ROLE_CREATE: my-group-id
                ROLE_DEPLOY: my-group-id
                ROLE_DESTROY: my-group-id
                ROLE_MANAGE: my-group-id
                ROLE_MODIFY: my-group-id
                ROLE_SCHEDULE: my-group-id
                ROLE_VIEW: my-group-id
----
====

You can also customize the role-mapping behavior by providing your own Spring bean definition that
extends Spring Cloud Data Flow's `AuthorityMapper` interface. In that case,
the custom bean definition takes precedence over the default one provided by
Spring Cloud Data Flow.

The default scheme uses seven roles to protect the xref:api-guide[REST endpoints]
that Spring Cloud Data Flow exposes:

* *ROLE_CREATE*: For anything that involves creating, such as creating streams or tasks
* *ROLE_DEPLOY*: For deploying streams or launching tasks
* *ROLE_DESTROY*: For anything that involves deleting streams, tasks, and so on.
* *ROLE_MANAGE*: For Boot management endpoints
* *ROLE_MODIFY*: For anything that involves mutating the state of the system
* *ROLE_SCHEDULE*: For scheduling related operation (such as scheduling a task)
* *ROLE_VIEW*: For anything that relates to retrieving state

As mentioned earlier in this section, all authorization-related default settings are specified
in `dataflow-server-defaults.yml`, which is part of the Spring Cloud Data Flow Core
Module. Nonetheless, you can override those settings, if desired -- for example,
in `application.yml`. The configuration takes the form of a YAML list (as some
rules may have precedence over others). Consequently, you need to copy and paste
the whole list and tailor it to your needs (as there is no way to merge lists).

NOTE: Always refer to your version of the `application.yml` file, as the following snippet may be outdated.

The default rules are as follows:

====
[source,yaml]
----
spring:
  cloud:
    dataflow:
      security:
        authorization:
          enabled: true
          loginUrl: "/"
          permit-all-paths: "/authenticate,/security/info,/assets/**,/dashboard/logout-success-oauth.html,/favicon.ico"
          rules:
            # About

            - GET    /about                          => hasRole('ROLE_VIEW')

            # Audit

            - GET /audit-records                     => hasRole('ROLE_VIEW')
            - GET /audit-records/**                  => hasRole('ROLE_VIEW')

            # Boot Endpoints

            - GET /management/**                  => hasRole('ROLE_MANAGE')

            # Apps

            - GET    /apps                           => hasRole('ROLE_VIEW')
            - GET    /apps/**                        => hasRole('ROLE_VIEW')
            - DELETE /apps/**                        => hasRole('ROLE_DESTROY')
            - POST   /apps                           => hasRole('ROLE_CREATE')
            - POST   /apps/**                        => hasRole('ROLE_CREATE')
            - PUT    /apps/**                        => hasRole('ROLE_MODIFY')

            # Completions

            - GET /completions/**                    => hasRole('ROLE_VIEW')

            # Job Executions & Batch Job Execution Steps && Job Step Execution Progress

            - GET    /jobs/executions                => hasRole('ROLE_VIEW')
            - PUT    /jobs/executions/**             => hasRole('ROLE_MODIFY')
            - GET    /jobs/executions/**             => hasRole('ROLE_VIEW')
            - GET    /jobs/thinexecutions            => hasRole('ROLE_VIEW')

            # Batch Job Instances

            - GET    /jobs/instances                 => hasRole('ROLE_VIEW')
            - GET    /jobs/instances/*               => hasRole('ROLE_VIEW')

            # Running Applications

            - GET    /runtime/streams                => hasRole('ROLE_VIEW')
            - GET    /runtime/streams/**             => hasRole('ROLE_VIEW')
            - GET    /runtime/apps                   => hasRole('ROLE_VIEW')
            - GET    /runtime/apps/**                => hasRole('ROLE_VIEW')

            # Stream Definitions

            - GET    /streams/definitions            => hasRole('ROLE_VIEW')
            - GET    /streams/definitions/*          => hasRole('ROLE_VIEW')
            - GET    /streams/definitions/*/related  => hasRole('ROLE_VIEW')
            - POST   /streams/definitions            => hasRole('ROLE_CREATE')
            - DELETE /streams/definitions/*          => hasRole('ROLE_DESTROY')
            - DELETE /streams/definitions            => hasRole('ROLE_DESTROY')

            # Stream Deployments

            - DELETE /streams/deployments/*          => hasRole('ROLE_DEPLOY')
            - DELETE /streams/deployments            => hasRole('ROLE_DEPLOY')
            - POST   /streams/deployments/**         => hasRole('ROLE_MODIFY')
            - GET    /streams/deployments/**         => hasRole('ROLE_VIEW')

            # Stream Validations

            - GET /streams/validation/               => hasRole('ROLE_VIEW')
            - GET /streams/validation/*              => hasRole('ROLE_VIEW')

            # Stream Logs
            - GET /streams/logs/*                    => hasRole('ROLE_VIEW')

            # Task Definitions

            - POST   /tasks/definitions              => hasRole('ROLE_CREATE')
            - DELETE /tasks/definitions/*            => hasRole('ROLE_DESTROY')
            - GET    /tasks/definitions              => hasRole('ROLE_VIEW')
            - GET    /tasks/definitions/*            => hasRole('ROLE_VIEW')

            # Task Executions

            - GET    /tasks/executions               => hasRole('ROLE_VIEW')
            - GET    /tasks/executions/*             => hasRole('ROLE_VIEW')
            - POST   /tasks/executions               => hasRole('ROLE_DEPLOY')
            - POST   /tasks/executions/*             => hasRole('ROLE_DEPLOY')
            - DELETE /tasks/executions/*             => hasRole('ROLE_DESTROY')

            # Task Schedules

            - GET    /tasks/schedules                => hasRole('ROLE_VIEW')
            - GET    /tasks/schedules/*              => hasRole('ROLE_VIEW')
            - GET    /tasks/schedules/instances      => hasRole('ROLE_VIEW')
            - GET    /tasks/schedules/instances/*    => hasRole('ROLE_VIEW')
            - POST   /tasks/schedules                => hasRole('ROLE_SCHEDULE')
            - DELETE /tasks/schedules/*              => hasRole('ROLE_SCHEDULE')

            # Task Platform Account List */

            - GET    /tasks/platforms                => hasRole('ROLE_VIEW')

            # Task Validations

            - GET    /tasks/validation/               => hasRole('ROLE_VIEW')
            - GET    /tasks/validation/*              => hasRole('ROLE_VIEW')

            # Task Logs
            - GET /tasks/logs/*                       => hasRole('ROLE_VIEW')

            # Tools

            - POST   /tools/**                       => hasRole('ROLE_VIEW')

----
====

The format of each line is the following:

====
----
HTTP_METHOD URL_PATTERN '=>' SECURITY_ATTRIBUTE
----

where:

* HTTP_METHOD is one HTTP method (such as PUT or GET), capital case.
* URL_PATTERN is an Ant-style URL pattern.
* SECURITY_ATTRIBUTE is a SpEL expression. See https://docs.spring.io/spring-security/site/docs/current/reference/htmlsingle/#el-access[Expression-Based Access Control].
* Each of those is separated by one or whitespace characters (spaces, tabs, and so on).
====

Be mindful that the above is a YAML list, not a map (thus the use of '-' dashes
at the start of each line) that lives under the `spring.cloud.dataflow.security.authorization.rules` key.

[[configuration-security-authorization-local-shell-and-dashboard]]
===== Authorization -- Shell and Dashboard Behavior

When security is enabled, the dashboard and the shell are role-aware,
meaning that, depending on the assigned roles, not all functionality may be visible.

For instance, shell commands for which the user does not have the necessary roles
are marked as unavailable.

[IMPORTANT]
====
Currently, the shell's `help` command lists commands that are unavailable.
Please track the following issue: https://github.com/spring-projects/spring-shell/issues/115
====

Conversely, for the Dashboard, the UI does not show pages or page elements for
which the user is not authorized.

[[configuration-security-securing-management-endpoints]]
===== Securing the Spring Boot Management Endpoints

When security is enabled, the
{spring-boot-docs}/#actuator.monitoring[Spring Boot HTTP Management Endpoints]
are secured in the same way as the other REST endpoints. The management REST endpoints
are available under `/management` and require the `MANAGEMENT` role.

The default configuration in `dataflow-server-defaults.yml` is as follows:

====
[source,yaml]
----
management:
  endpoints:
    web:
      base-path: /management
  security:
    roles: MANAGE
----
====

IMPORTANT: Currently, you should not customize the default management path.

[[configuration-security-uaa-authentication]]
==== Setting up UAA Authentication

For local deployment scenarios, we recommend using the https://github.com/cloudfoundry/uaa[CloudFoundry User
Account and Authentication (UAA) Server], which is https://openid.net/certification/[OpenID certified].
While the UAA is used by https://www.cloudfoundry.org/[Cloud Foundry],
it is also a fully featured stand alone OAuth2 server with enterprise features, such as
https://github.com/cloudfoundry/uaa/blob/develop/docs/UAA-LDAP.md[LDAP integration].

===== Requirements

You need to check out, build and run UAA. To do so, make sure that you:

- Use Java 8.
- Have https://git-scm.com/[Git] installed.
- Have the https://github.com/cloudfoundry/cf-uaac[CloudFoundry UAA Command Line Client] installed.
- Use a different host name for UAA when running on the same machine -- for example, `http://uaa/`.

If you run into issues installing _uaac_, you may have to set the `GEM_HOME` environment
variable:

====
[source,bash]
----
export GEM_HOME="$HOME/.gem"
----
====

You should also ensure that `~/.gem/gems/cf-uaac-4.2.0/bin` has been added to your path.

===== Prepare UAA for JWT

As the UAA is an OpenID provider and uses JSON Web Tokens (JWT), it needs to have
a private key for signing those JWTs:

====
[source,bash]
----
openssl genrsa -out signingkey.pem 2048
openssl rsa -in signingkey.pem -pubout -out verificationkey.pem
export JWT_TOKEN_SIGNING_KEY=$(cat signingkey.pem)
export JWT_TOKEN_VERIFICATION_KEY=$(cat verificationkey.pem)
----
====

Later, once the UAA is started, you can see the keys when you access `http://uaa:8080/uaa/token_keys`.

NOTE: Here, the `uaa` in the URL `http://uaa:8080/uaa/token_keys` is the hostname.

===== Download and Start UAA

To download and install UAA, run the following commands:

====
[source,bash]
----
git clone https://github.com/pivotal/uaa-bundled.git
cd uaa-bundled
./mvnw clean install
java -jar target/uaa-bundled-1.0.0.BUILD-SNAPSHOT.jar
----
====

The configuration of the UAA is driven by a YAML file `uaa.yml`, or you can script the configuration
using the UAA Command Line Client:

====
[source,bash]
----
uaac target http://uaa:8080/uaa
uaac token client get admin -s adminsecret
uaac client add dataflow \
  --name dataflow \
  --secret dataflow \
  --scope cloud_controller.read,cloud_controller.write,openid,password.write,scim.userids,sample.create,sample.view,dataflow.create,dataflow.deploy,dataflow.destroy,dataflow.manage,dataflow.modify,dataflow.schedule,dataflow.view \
  --authorized_grant_types password,authorization_code,client_credentials,refresh_token \
  --authorities uaa.resource,dataflow.create,dataflow.deploy,dataflow.destroy,dataflow.manage,dataflow.modify,dataflow.schedule,dataflow.view,sample.view,sample.create\
  --redirect_uri http://localhost:9393/login \
  --autoapprove openid

uaac group add "sample.view"
uaac group add "sample.create"
uaac group add "dataflow.view"
uaac group add "dataflow.create"

uaac user add springrocks -p mysecret --emails springrocks@someplace.com
uaac user add vieweronly -p mysecret --emails mrviewer@someplace.com

uaac member add "sample.view" springrocks
uaac member add "sample.create" springrocks
uaac member add "dataflow.view" springrocks
uaac member add "dataflow.create" springrocks
uaac member add "sample.view" vieweronly
----
====

The preceding script sets up the dataflow client as well as two users:

- User _springrocks_ has have both scopes: `sample.view` and `sample.create`.
- User _vieweronly_ has only one scope: `sample.view`.

Once added, you can quickly double-check that the UAA has the users created:

====
[source,bash]
----
curl -v -d"username=springrocks&password=mysecret&client_id=dataflow&grant_type=password" -u "dataflow:dataflow" http://uaa:8080/uaa/oauth/token -d 'token_format=opaque'
----
====

The preceding command should produce output similar to the following:

====
[source,bash]
----
*   Trying 127.0.0.1...
* TCP_NODELAY set
* Connected to uaa (127.0.0.1) port 8080 (#0)
* Server auth using Basic with user 'dataflow'
> POST /uaa/oauth/token HTTP/1.1
> Host: uaa:8080
> Authorization: Basic ZGF0YWZsb3c6ZGF0YWZsb3c=
> User-Agent: curl/7.54.0
> Accept: */*
> Content-Length: 97
> Content-Type: application/x-www-form-urlencoded
>
* upload completely sent off: 97 out of 97 bytes
< HTTP/1.1 200
< Cache-Control: no-store
< Pragma: no-cache
< X-XSS-Protection: 1; mode=block
< X-Frame-Options: DENY
< X-Content-Type-Options: nosniff
< Content-Type: application/json;charset=UTF-8
< Transfer-Encoding: chunked
< Date: Thu, 31 Oct 2019 21:22:59 GMT
<
* Connection #0 to host uaa left intact
{"access_token":"0329c8ecdf594ee78c271e022138be9d","token_type":"bearer","id_token":"eyJhbGciOiJSUzI1NiIsImprdSI6Imh0dHBzOi8vbG9jYWxob3N0OjgwODAvdWFhL3Rva2VuX2tleXMiLCJraWQiOiJsZWdhY3ktdG9rZW4ta2V5IiwidHlwIjoiSldUIn0.eyJzdWIiOiJlZTg4MDg4Ny00MWM2LTRkMWQtYjcyZC1hOTQ4MmFmNGViYTQiLCJhdWQiOlsiZGF0YWZsb3ciXSwiaXNzIjoiaHR0cDovL2xvY2FsaG9zdDo4MDkwL3VhYS9vYXV0aC90b2tlbiIsImV4cCI6MTU3MjYwMDE3OSwiaWF0IjoxNTcyNTU2OTc5LCJhbXIiOlsicHdkIl0sImF6cCI6ImRhdGFmbG93Iiwic2NvcGUiOlsib3BlbmlkIl0sImVtYWlsIjoic3ByaW5ncm9ja3NAc29tZXBsYWNlLmNvbSIsInppZCI6InVhYSIsIm9yaWdpbiI6InVhYSIsImp0aSI6IjAzMjljOGVjZGY1OTRlZTc4YzI3MWUwMjIxMzhiZTlkIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImNsaWVudF9pZCI6ImRhdGFmbG93IiwiY2lkIjoiZGF0YWZsb3ciLCJncmFudF90eXBlIjoicGFzc3dvcmQiLCJ1c2VyX25hbWUiOiJzcHJpbmdyb2NrcyIsInJldl9zaWciOiJlOTkyMDQxNSIsInVzZXJfaWQiOiJlZTg4MDg4Ny00MWM2LTRkMWQtYjcyZC1hOTQ4MmFmNGViYTQiLCJhdXRoX3RpbWUiOjE1NzI1NTY5Nzl9.bqYvicyCPB5cIIu_2HEe5_c7nSGXKw7B8-reTvyYjOQ2qXSMq7gzS4LCCQ-CMcb4IirlDaFlQtZJSDE-_UsM33-ThmtFdx--TujvTR1u2nzot4Pq5A_ThmhhcCB21x6-RNNAJl9X9uUcT3gKfKVs3gjE0tm2K1vZfOkiGhjseIbwht2vBx0MnHteJpVW6U0pyCWG_tpBjrNBSj9yLoQZcqrtxYrWvPHaa9ljxfvaIsOnCZBGT7I552O1VRHWMj1lwNmRNZy5koJFPF7SbhiTM8eLkZVNdR3GEiofpzLCfoQXrr52YbiqjkYT94t3wz5C6u1JtBtgc2vq60HmR45bvg","refresh_token":"6ee95d017ada408697f2d19b04f7aa6c-r","expires_in":43199,"scope":"scim.userids openid sample.create cloud_controller.read password.write cloud_controller.write sample.view","jti":"0329c8ecdf594ee78c271e022138be9d"}
----
====

By using the `token_format` parameter, you can request the token to be either:

- opaque
- jwt

include::configuration-local.adoc[]
include::configuration-cloudfoundry.adoc[]
include::configuration-kubernetes.adoc[]
include::configuration-carvel.adoc[]
